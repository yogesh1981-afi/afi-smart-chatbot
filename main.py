from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer # Use sentence-transformers library
from pinecone import Pinecone
import os

app = FastAPI()

# --- Environment Variable Loading ---
# Best practice: Load environment variables early and validate
PINECONE_API_KEY = os.environ.get("PINECONE_API_KEY")
PINECONE_ENVIRONMENT = os.environ.get("PINECONE_ENVIRONMENT") # Although not explicitly used by Pinecone client v3+, good to have
INDEX_NAME = "afi-index" # Consider making this an env variable too

if not PINECONE_API_KEY:
    # Critical error if Pinecone key is missing, stop the application
    raise ValueError("PINECONE_API_KEY environment variable must be set.")

# --- Model and Client Initialization ---
try:
    # Initialize Pinecone client (API key is mandatory)
    # Environment is often auto-detected or can be configured if needed
    pc = Pinecone(api_key=PINECONE_API_KEY)

    # Connect to the Pinecone index
    # Add error handling in case the index doesn't exist or connection fails
    index = pc.Index(INDEX_NAME)
    print(f"Successfully connected to Pinecone index: {INDEX_NAME}")

    # Load the SentenceTransformer model with the CORRECT identifier
    # This model is suitable for generating 384-dimensional embeddings
    model_id = 'sentence-transformers/all-MiniLM-L6-v2'
    model = SentenceTransformer(model_id)
    print(f"Successfully loaded SentenceTransformer model: {model_id}")

except Exception as e:
    # Catch initialization errors (e.g., invalid API key, network issues, model loading failure)
    print(f"FATAL: Error during initialization: {e}")
    # Depending on deployment, might want to exit or raise a more specific startup error
    raise RuntimeError(f"Failed to initialize resources: {e}") from e

# --- API Routes ---

# Health check route
@app.get("/ping")
async def ping():
    """Simple health check endpoint."""
    return {"message": "Embedding API is live"}

# Root route
@app.get("/")
async def read_root():
    """Root endpoint providing basic API information."""
    return {"message": "AFI Smart Chat Assistant Embedding API is running ðŸš€"}

# Pydantic model for the request body of the /embed endpoint
class EmbedRequest(BaseModel):
    text: str

# Pydantic model for the response body of the /embed endpoint
class EmbedResponse(BaseModel):
    embedding: list[float]

# Embed endpoint - CORRECTED LOGIC
@app.post("/embed", response_model=EmbedResponse)
async def embed_text(request: EmbedRequest):
    """
    Generates a vector embedding for the provided text using the loaded SentenceTransformer model.
    Does NOT upsert the embedding to Pinecone in this endpoint.
    """
    if not request.text:
        raise HTTPException(status_code=400, detail="Input text cannot be empty.")

    try:
        # Generate embedding using the loaded SentenceTransformer model
        #.tolist() converts the numpy array to a standard Python list suitable for JSON responses
        embedding = model.encode(request.text).tolist()

        # Return the generated embedding
        return {"embedding": embedding}

    except Exception as e:
        # Log the error for debugging purposes (consider using a proper logger)
        print(f"Error during embedding generation: {e}")
        # Return a generic server error to the client
        raise HTTPException(status_code=500, detail=f"Failed to generate embedding: {str(e)}")

# --- Optional: Add a Query Endpoint (Example) ---
# If you need to query Pinecone using an embedding generated by this API,
# you would typically do that in a separate function or endpoint in your main application logic,
# not within the /embed endpoint itself. Here's a conceptual example (not fully implemented):

# class QueryRequest(BaseModel):
#     text: str
#     top_k: int = 5 # Number of results to return

# class QueryResponse(BaseModel):
#     matches: list # Define a more specific model for matches if needed

# @app.post("/query", response_model=QueryResponse)
# async def query_index(request: QueryRequest):
#     """
#     Embeds the query text and searches the Pinecone index for similar vectors.
#     """
#     if not request.text:
#         raise HTTPException(status_code=400, detail="Query text cannot be empty.")
#
#     try:
#         # 1. Embed the query text
#         query_embedding = model.encode(request.text).tolist()
#
#         # 2. Query Pinecone
#         query_response = index.query(
#             vector=query_embedding,
#             top_k=request.top_k,
#             include_metadata=True # Example: Include metadata if stored
#         )
#
#         # 3. Process and return results
#         # (Format the response as needed based on query_response structure)
#         matches = query_response.get('matches',)
#         return {"matches": matches}
#
#     except Exception as e:
#         print(f"Error during query: {e}")
#         raise HTTPException(status_code=500, detail=f"Failed to query index: {str(e)}")

# --- Run with Uvicorn (if running directly) ---
# if __name__ == "__main__":
#     import uvicorn
#     # Ensure PINECONE_API_KEY is set in your environment before running
#     # Example: PINECONE_API_KEY="your-key" python main.py
#     if not PINECONE_API_KEY:
#         print("Error: PINECONE_API_KEY environment variable not set.")
#     else:
#         uvicorn.run(app, host="0.0.0.0", port=8000)